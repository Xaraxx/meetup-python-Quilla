{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Una breve introducción al análisis de datos con Jupyter\n",
    "\n",
    ">\"Un programador es la persona considerada experta en ser capaz de sacar, después de innumerables tecleos, una serie infinita de respuestas incomprensibles calculadas con precisión micrométrica a partir de vagas asunciones basadas en discutibles cifras tomadas de documentos inconcluyentes y llevados a cabo con instrumentos de escasa precisión, por personas de fiabilidad dudosa y cuestionable mentalidad con el propósito declarado de molestar y confundiar al desesperado e indefenso departamento que tuvo la mala fortuna de pedir la información en primer lugar.\"  \n",
    "\n",
    "_IEEE Grid newsmagazine_\n",
    "\n",
    "## ¿Qué es o qué se entiende por análisis de datos? \n",
    "\n",
    "1. Álgebra lineal-álgebra lineal numérica \n",
    "1. Modelado estadístico\n",
    "1. Visualización\n",
    "1. Lingüística computacional\n",
    "1. Análisis de gráficos\n",
    "1. Machine learning\n",
    "1. Inteligencia comercial\n",
    "1. almacenamiento y recuperación de datos\n",
    "1. Otras...\n",
    "\n",
    "## ¿Qué es o qué son los _datos_?\n",
    "\n",
    "Será que esto es un dato? \n",
    "\n",
    " ![title](img/hubble.jpg.jpg)\n",
    " \n",
    " O esto?\n",
    " \n",
    " ![title](img/figure-65.png)\n",
    "\n",
    "Qué tal esto? \n",
    "\n",
    " ![title](img/queen-i-want-to-break-free-works.jpg)\n",
    " \n",
    " Mmm... y esto?\n",
    " \n",
    " ![title](img/fisic3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, \n",
    ">...las cantidades, caracteres o símbolos en los que las operaciones son realizadas por una computadora, que pueden almacenarse y transmitirse en forma de señales eléctricas y grabadas en medios de grabación magnéticos, ópticos o mecánicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasos para el análisis de datos\n",
    "\n",
    "Cómo se realiza el análisis de datos?\n",
    "\n",
    "### 1. Interacción con el mundo exterior \n",
    "\n",
    "### 2. Preparación \n",
    "\n",
    "   Limpiar, cortar, combinar, normalizar, remodelar, todas las transformaciones que implican el pre-procesamiento de datos para el análisis.\n",
    "\n",
    "### 3. Transformación \n",
    "\n",
    "   Aplicación de las operaciones matemáticas y estadísticas a un grupo de datos para la derivación de nuevos conjuntos. Ejemplo, organizar una serie de datos de cualquier cosa en forma matricial.\n",
    "\n",
    "### 4. Modelado y cálculo \n",
    "\n",
    "   Conectar estos datos a modelos matemáticos, estadísticos, algoritmos de aprendizaje automático, u otras  *herramientas computacionales*. \n",
    "    \n",
    "#### _Herramientas que Python ofrece para el procesamiento de datos_ \n",
    "\n",
    "1. Numpy\n",
    "\n",
    "1. Scipy\n",
    "\n",
    "1. Pandas\n",
    "\n",
    "1.  Scikit-learn\n",
    "\n",
    "1. Matplotlib\n",
    "\n",
    "1. IPython - Jupyter notebook \n",
    "\n",
    "_Scikit-learn_ es el núcleo de la ciencia de datos  en Python. Ofrece todo lo que puede necesitar en términos de pre-procesamiento de datos,\n",
    "aprendizaje supervisado y no supervisado, selección del modelo, validación y errormétrica. Espere que hablemos extensamente sobre este paquete a lo largo de este libro. _Scikit-learn_ comenzó en 2007 como un proyecto Google Summer of Code por David Cournapeau. Desde 2013, ha sido asumido por los investigadores del INRA (Instituto Francés de Investigación en Ciencias de la Computación y Automatización). [@boschetti2015python]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora veamos unos ejemplos...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1: Manipulación de un conjunto de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "iris_filename = 'iris.csv'\n",
    "\n",
    "iris_dataset = pd.read_csv(iris_filename, sep =',', decimal ='.',header=None,\n",
    "names= ['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
    "'target'])\n",
    "\n",
    "#iris_dataset\n",
    "\n",
    "\n",
    "#iris_dataset.head()\n",
    "#iris_dataset.tail()\n",
    "#iris_dataset.columns\n",
    "\n",
    "Y = iris_dataset['target']\n",
    "\n",
    "X = iris_dataset[['sepal_length', 'sepal_width']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2:  Tratamiento para grandes cantidades de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iris_chunks = pd.read_csv(iris_filename, header=None,\n",
    "names=['C1', 'C2', 'C3', 'C4', 'C5'], chunksize=10)\n",
    "#for chunk in iris_chunks:\n",
    "#    print(chunk.shape)\n",
    "#    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n",
      "(20, 5)\n"
     ]
    }
   ],
   "source": [
    "iris_iterator = pd.read_csv(iris_filename, header=None,\n",
    "names=['C1', 'C2', 'C3', 'C4', 'C5'], iterator=True)\n",
    "print(iris_iterator.get_chunk(10).shape)\n",
    "print(iris_iterator.get_chunk(20).shape)\n",
    "piece = iris_iterator.get_chunk(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 3: *Un poco* de pre-procesamiento..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mask_feature = iris_dataset['sepal_length'] > 6.0\n",
    "mask_target = iris_dataset['target'] == 'Iris-virginica'\n",
    "iris_dataset.loc[mask_target, 'target'] = 'newlabel'\n",
    "iris_dataset['target'].unique()\n",
    "#mask_feature\n",
    "#mask_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 4: Determinación de una propiedad física apartir de un conjunto de datos experimentales\n",
    "\n",
    " ![title](img/dens.png)\n",
    " \n",
    " Bien! sea $\\rho$ la densidad del etilenglicol, entonces se define una función $\\rho(T)$ la cual es linealmente dependiente de la temperatura, es decir:\n",
    " \n",
    " $$\\rho(T) = \\pmb{A*T} $$\n",
    " \n",
    " En la cual $A$ es una constante, que se pretende determinar  con el fin de establecer la relación funcional entre la densidad y la temperatura del compuesto. Además, nótese que se consideran mezclas de distinta composición por lo cual se deben calcular distintos valores de la misma de acuerdo a la composición considerada, finalmente; las expresiones para  \n",
    " \n",
    " $$ \\rho(T) = \n",
    " \\left [\n",
    "\\begin{matrix}\n",
    "\\rho_{11} & \\rho_{12} & \\rho_{13} & \\dots  & \\rho_{1n} \\\\\n",
    "    \\rho_{21} & \\rho_{22} & \\rho_{23} & \\dots  & \\rho_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\rho_{d1} & \\rho_{d2} & \\rho_{d3} & \\dots  & \\rho_{dn}\n",
    "\\end{matrix}\n",
    "\\right ]\n",
    "$$\n",
    "\n",
    " $$ T = \n",
    " \\left [\n",
    "\\begin{matrix}\n",
    "T_{11} \\\\\n",
    "T_{21} \\\\\n",
    "\\vdots \\\\\n",
    "T_{d1} \n",
    "\\end{matrix}\n",
    "\\right ]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gudarjs/.local/share/virtualenvs/xarax-1E43_9BV/lib/python3.6/site-packages/ipykernel_launcher.py:12: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg as lin \n",
    "\n",
    "experimental_data = pd.read_csv(\"density.csv\").values\n",
    "#experimental_data.head(2)\n",
    "\n",
    "rho = experimental_data[: , 1:]\n",
    "T   = experimental_data[:,0]\n",
    "t   = np.array(T)[np.newaxis]\n",
    "T1  = t.T\n",
    "p   = lin.lstsq(T1,rho)[0]\n",
    "\n",
    "#T1  = T.transpose()\n",
    "#rho1= rho.transpose()\n",
    "#T_t = T.T\n",
    "#sol = (T1//rho)\n",
    "\n",
    "\n",
    "#print(p)\n",
    "#print(rho)\n",
    "#print(T1.shape)\n",
    "\n",
    "#print(T1)\n",
    "\n",
    "#print(experimental_data)\n",
    "#print(sol)\n",
    "#print(T)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1243.6783854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1243.6783854"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#T = 2\n",
    "d = 0;\n",
    "\n",
    "def density(T):\n",
    "    d = 30.35567708*T + 788.3432292000001 \n",
    "    print(d) \n",
    "    return d\n",
    "\n",
    "density(15)\n",
    "\n",
    "#1091.9 - density(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAGAMOS MÁS CÁLCULOS!\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 3.842810589450494\n",
      "466 µs ± 5.88 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "boston = load_boston() \n",
    "bos = pd.DataFrame(boston.data)\n",
    "#bos.head()\n",
    "boston.target[:5]\n",
    "\n",
    "bos['PRICE'] = boston.target\n",
    "\n",
    "X = bos.drop('PRICE', axis = 1)\n",
    "\n",
    "#print(boston.DESCR)\n",
    "\n",
    "\n",
    "#print('Estimated intercept coefficient:', lm.intercept_)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data,\n",
    "boston.target, test_size=0.2, random_state=0)\n",
    "\n",
    "regr = LinearRegression()\n",
    "regr\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "print(\"MAE\", mean_absolute_error(y_test, y_pred))  \n",
    "\n",
    "%timeit regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
